{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation Walkthrough\n",
    "\n",
    "this notebook contains a note while toying with data and the result, also the steps to produce the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "\n",
    "LABEL = ['Standing still', 'Sitting and relaxing', 'Lying down', 'Walking', 'Climbing', 'Running']\n",
    "N_WORKER = 8\n",
    "BATCH_SIZE = 20\n",
    "VALID_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)  # hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "workers = connect_to_workers(n_workers = N_WORKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "1. Split files detected by glob to train and test\n",
    "2. Read CSV\n",
    "3. Construct Dataset class\n",
    "\n",
    "For the first experiment, i will try to use all features that available so the input will be [1 x 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "# folder path\n",
    "PATH = 'data/Preprocessed'\n",
    "TRAIN_FILES_COUNT = 8\n",
    "# seed for random\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(PATH+\"/*.csv\")\n",
    "\n",
    "# split into train and test\n",
    "# according to note, only 8 data for training, and the rest for testing\n",
    "count_valid = math.ceil(VALID_SIZE*TRAIN_FILES_COUNT)\n",
    "random.shuffle(files)\n",
    "valid_files,train_files, test_files = files[0:count_valid],files[count_valid:TRAIN_FILES_COUNT],files[TRAIN_FILES_COUNT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Preprocessed\\mHealth_subject2.csv\n",
      "types of label inside: [5, 1, 2, 3, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "index_to_read = 3\n",
    "sample_read = []\n",
    "sample_label = []\n",
    "label = []\n",
    "\n",
    "with open(train_files[index_to_read]) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if int(row[21]) != 0:\n",
    "            if int(row[21]) not in label:\n",
    "                label.append(int(row[21]))\n",
    "            sample_read.append([float(item) for item in row[0:21]])\n",
    "            sample_label.append(int(row[21]))\n",
    "\n",
    "print(train_files[index_to_read])\n",
    "print('types of label inside: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chest \t\t\tLeft Angkle\n",
      "-9.15 \t0.64 \t0.90 \t0.18 \t-9.49 \t-6.48 \t-0.27 \t-0.39 \t0.17 \t-94.93 \t-29.88 \t11.43\n",
      "Label \t\t\tRight Angkle\n",
      "5 \t\t\t-3.45 \t-10.68 \t2.34 \t-0.41 \t-0.40 \t0.01 \t5.76 \t19.53 \t-1.16\n",
      "\n",
      "Chest \t\t\tLeft Angkle\n",
      "-8.62 \t0.36 \t1.05 \t0.82 \t-7.38 \t-3.79 \t-0.34 \t-0.50 \t0.01 \t-80.26 \t-37.18 \t9.77\n",
      "Label \t\t\tRight Angkle\n",
      "1 \t\t\t-3.69 \t-10.95 \t1.98 \t-0.37 \t-0.41 \t-0.05 \t7.69 \t14.47 \t20.59\n",
      "\n",
      "Chest \t\t\tLeft Angkle\n",
      "-7.90 \t0.03 \t0.67 \t0.72 \t-4.94 \t-1.20 \t-0.34 \t-0.50 \t0.01 \t-61.56 \t-19.20 \t6.19\n",
      "Label \t\t\tRight Angkle\n",
      "2 \t\t\t-3.59 \t-10.21 \t2.15 \t-0.37 \t-0.41 \t-0.05 \t10.89 \t11.56 \t41.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print sample data\n",
    "for index,rep in enumerate(sample_read[:3]):\n",
    "    print('Chest \\t\\t\\tLeft Angkle')\n",
    "    print('{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f}'.format(*rep))\n",
    "    print('Label \\t\\t\\tRight Angkle')\n",
    "    print('{} \\t\\t\\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f} \\t{:.2f}'.format(label[index],*rep[12:]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Constructed Dataset\n",
    "\n",
    "To make it easy, we create a dataloader for reading and returning value from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 129024\n",
      "valid dataset: 18432\n",
      "test dataset: 36864\n"
     ]
    }
   ],
   "source": [
    "from dataloader import ImuPoseDataset\n",
    "\n",
    "train_dataset = ImuPoseDataset(files=train_files)\n",
    "valid_dataset = ImuPoseDataset(files=valid_files)\n",
    "test_dataset = ImuPoseDataset(files=test_files)\n",
    "\n",
    "print(\"train dataset: {}\".format(len(train_dataset)))\n",
    "print(\"valid dataset: {}\".format(len(valid_dataset)))\n",
    "print(\"test dataset: {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Dataloader\n",
    "\n",
    "To iterate our dataset we will use Pytorch built-in dataloader. Since we are training with federated learning we will need to transform it into federatedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( train_dataset\n",
    "                                                .federate(workers), # <-- we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           batch_size=BATCH_SIZE)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
